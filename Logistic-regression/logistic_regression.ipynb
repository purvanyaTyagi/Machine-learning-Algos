{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_N7DwP3NkaLm"
      },
      "outputs": [],
      "source": [
        "from torch import *\n",
        "import torch\n",
        "import pandas as pd\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CpRg-r0dJGg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = pd.read_csv('advertising.csv')\n",
        "data.head()\n",
        "\n",
        "X = data.drop(['Timestamp', 'Clicked on Ad', 'Ad Topic Line', 'Country', 'City'], axis=1)\n",
        "y = data['Clicked on Ad']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "num_columns = ['Daily Time Spent on Site', 'Age', 'Area Income', 'Daily Internet Usage', 'Male']\n",
        "\n",
        "\n",
        "ct = make_column_transformer(\n",
        "    (MinMaxScaler(), num_columns),\n",
        "    (StandardScaler(), num_columns),\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "X_train = ct.fit_transform(X_train)\n",
        "X_test = ct.transform(X_test)\n",
        "\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "ones_column = torch.ones(700, 1)\n",
        "ones_column_test = torch.ones(300, 1)\n",
        "X_train = torch.cat((X_train, ones_column), dim=1)\n",
        "X_train = X_train.to(device)\n",
        "X_test = torch.cat((X_test, ones_column_test), dim=1)\n",
        "X_test = X_test.to(device)\n",
        "y_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
        "y_train = y_train.unsqueeze(0)\n",
        "y_train = y_train.to(device)"
      ],
      "metadata": {
        "id": "ag1KuJ0VlBS-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-j0UvgLNWwZc",
        "outputId": "0359a008-3d6a-4fb7-fa0a-16d566c48eac"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.7318,  0.4762,  0.7639,  ...,  1.5211,  1.0438,  1.0000],\n",
              "        [ 0.2285,  0.3095,  0.7865,  ..., -0.7576, -0.9580,  1.0000],\n",
              "        [ 0.6259,  0.1429,  0.7909,  ...,  0.7343, -0.9580,  1.0000],\n",
              "        ...,\n",
              "        [ 0.9990,  0.6190,  0.5791,  ...,  0.0339,  1.0438,  1.0000],\n",
              "        [ 0.4090,  0.5476,  0.8962,  ..., -0.9514, -0.9580,  1.0000],\n",
              "        [ 0.9742,  0.5000,  0.6963,  ...,  0.2978, -0.9580,  1.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_length = len(X_train[0])\n",
        "\n",
        "weights = torch.zeros((feature_length, 1), device=device, requires_grad=True)"
      ],
      "metadata": {
        "id": "OvQJT1cjlUk_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sig = torch.nn.Sigmoid()\n",
        "for epoch in range(0, 20):\n",
        "  out = sig(torch.matmul(torch.t(weights), X_train .T))\n",
        "  loss = torch.nn.functional.binary_cross_entropy(out, y_train.float())\n",
        "  grad = torch.autograd.grad(loss, weights, create_graph=True)[0]\n",
        "  hessian = torch.zeros(len(weights), len(weights))  # Initialize Hessian matrix\n",
        "  for i in range(0, 10):\n",
        "      # Compute second derivative of loss w.r.t. params[i] and params[j]\n",
        "      grad2 = torch.autograd.grad(grad[i.long()], weights, retain_graph=True)[0]\n",
        "      hessian[i.long()] = grad2.squeeze()\n",
        "  hessian += 1e-5 * torch.eye(len(weights))\n",
        "  hessian = torch.linalg.inv(hessian)\n",
        "  hessian = hessian.to(device)\n",
        "  update = torch.matmul(hessian, grad)\n",
        "  weights = weights - update.detach()\n",
        "  print('--------------------------------------------------------------------------------------------------')\n",
        "  print('epoch number: ')\n",
        "  print(epoch)\n",
        "  print('loss: ')\n",
        "  print(loss)\n",
        "  print('--------------------------------------------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87Xt2An53TUc",
        "outputId": "3bf8d48c-e772-4911-9c98-8346ba6d1b38"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------------------------\n",
            "epoch number: \n",
            "tensor(0.)\n",
            "loss: \n",
            "tensor(0.6931, grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "--------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------\n",
            "epoch number: \n",
            "tensor(1.)\n",
            "loss: \n",
            "tensor(0.2142, grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "--------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------\n",
            "epoch number: \n",
            "tensor(2.)\n",
            "loss: \n",
            "tensor(0.1313, grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "--------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------\n",
            "epoch number: \n",
            "tensor(3.)\n",
            "loss: \n",
            "tensor(0.0989, grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "--------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------\n",
            "epoch number: \n",
            "tensor(4.)\n",
            "loss: \n",
            "tensor(0.0845, grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "--------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------\n",
            "epoch number: \n",
            "tensor(5.)\n",
            "loss: \n",
            "tensor(0.0798, grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "--------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------\n",
            "epoch number: \n",
            "tensor(6.)\n",
            "loss: \n",
            "tensor(0.0792, grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "--------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------\n",
            "epoch number: \n",
            "tensor(7.)\n",
            "loss: \n",
            "tensor(0.0792, grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "--------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------\n",
            "epoch number: \n",
            "tensor(8.)\n",
            "loss: \n",
            "tensor(0.0792, grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "--------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------\n",
            "epoch number: \n",
            "tensor(9.)\n",
            "loss: \n",
            "tensor(0.0792, grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "--------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------\n",
            "epoch number: \n",
            "tensor(10.)\n",
            "loss: \n",
            "tensor(0.0792, grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "--------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------\n",
            "epoch number: \n",
            "tensor(11.)\n",
            "loss: \n",
            "tensor(0.0792, grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "--------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------\n",
            "epoch number: \n",
            "tensor(12.)\n",
            "loss: \n",
            "tensor(0.0792, grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "--------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------\n",
            "epoch number: \n",
            "tensor(13.)\n",
            "loss: \n",
            "tensor(0.0792, grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "--------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------\n",
            "epoch number: \n",
            "tensor(14.)\n",
            "loss: \n",
            "tensor(0.0792, grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "--------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------\n",
            "epoch number: \n",
            "tensor(15.)\n",
            "loss: \n",
            "tensor(0.0792, grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "--------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------\n",
            "epoch number: \n",
            "tensor(16.)\n",
            "loss: \n",
            "tensor(0.0792, grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "--------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------\n",
            "epoch number: \n",
            "tensor(17.)\n",
            "loss: \n",
            "tensor(0.0792, grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "--------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------\n",
            "epoch number: \n",
            "tensor(18.)\n",
            "loss: \n",
            "tensor(0.0792, grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "--------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------\n",
            "epoch number: \n",
            "tensor(19.)\n",
            "loss: \n",
            "tensor(0.0792, grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "--------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------\n",
            "epoch number: \n",
            "tensor(20.)\n",
            "loss: \n",
            "tensor(0.0792, grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "--------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-ae94a539f1bb>:2: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  for epoch in range(0, 20):\n",
            "<ipython-input-20-ae94a539f1bb>:7: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  for i in range(0, 10):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = sig(torch.matmul(torch.t(weights), X_test.T))\n",
        "out"
      ],
      "metadata": {
        "id": "6FkeB4o_hwjU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30ab6f59-8ce1-485d-f55b-d3389233ed21"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[9.9268e-01, 9.9997e-01, 9.9973e-01, 9.9929e-01, 5.5811e-03, 5.2373e-01,\n",
              "         9.8450e-03, 9.9996e-01, 4.7517e-03, 1.0000e+00, 1.9650e-03, 9.9975e-01,\n",
              "         9.9999e-01, 6.4551e-03, 9.9998e-01, 1.0000e+00, 1.0000e+00, 9.9957e-01,\n",
              "         2.5048e-02, 9.9999e-01, 1.6771e-03, 9.9680e-01, 1.0000e+00, 8.7385e-02,\n",
              "         1.4586e-03, 1.0000e+00, 1.6268e-02, 1.1782e-03, 9.9999e-01, 9.9967e-01,\n",
              "         1.3908e-01, 4.5808e-01, 3.5969e-03, 9.9372e-01, 1.0000e+00, 4.8690e-03,\n",
              "         2.3630e-03, 6.5530e-01, 9.9997e-01, 1.0000e+00, 1.1582e-02, 9.9999e-01,\n",
              "         6.5877e-01, 1.0000e+00, 5.1881e-03, 4.3709e-02, 9.9076e-01, 1.0000e+00,\n",
              "         2.3321e-01, 9.9995e-01, 1.0000e+00, 2.6049e-02, 9.9975e-01, 9.1250e-03,\n",
              "         2.4150e-03, 6.8734e-03, 1.0090e-02, 1.0000e+00, 9.5453e-03, 9.9998e-01,\n",
              "         3.6078e-03, 9.9972e-01, 1.4036e-02, 1.0000e+00, 9.9884e-01, 9.9992e-01,\n",
              "         1.0000e+00, 4.6506e-03, 9.9940e-01, 9.9996e-01, 9.9975e-01, 9.9931e-01,\n",
              "         9.1523e-01, 3.0990e-02, 8.9610e-01, 5.7707e-01, 1.6087e-03, 1.0000e+00,\n",
              "         9.9961e-01, 2.9520e-03, 1.0000e+00, 9.9991e-01, 9.9997e-01, 9.9863e-01,\n",
              "         1.7755e-02, 2.5079e-03, 9.3458e-01, 1.3379e-02, 2.9662e-02, 2.6287e-03,\n",
              "         9.7513e-02, 9.9959e-01, 2.8694e-03, 9.9898e-01, 1.5168e-01, 2.8178e-01,\n",
              "         1.0000e+00, 9.9997e-01, 1.4024e-02, 1.1593e-03, 9.9991e-01, 6.3217e-03,\n",
              "         9.9999e-01, 2.3025e-03, 9.9999e-01, 3.2823e-02, 9.9994e-01, 8.7117e-02,\n",
              "         9.9998e-01, 3.6272e-03, 1.0000e+00, 1.7991e-03, 9.9315e-01, 9.9999e-01,\n",
              "         1.3758e-02, 8.1430e-01, 8.1913e-03, 1.8738e-01, 7.0301e-03, 1.4654e-01,\n",
              "         9.9769e-01, 9.9787e-01, 1.0000e+00, 5.9246e-03, 5.6141e-01, 1.0389e-03,\n",
              "         9.2006e-01, 2.4050e-03, 4.3081e-02, 9.9703e-01, 9.9814e-01, 9.9998e-01,\n",
              "         9.9039e-01, 3.3625e-03, 3.8609e-01, 3.9947e-02, 2.1691e-03, 9.9410e-01,\n",
              "         2.7131e-03, 9.3934e-01, 9.0114e-03, 1.3377e-01, 9.9966e-01, 9.9968e-01,\n",
              "         1.4546e-02, 9.9997e-01, 9.9960e-01, 3.9454e-02, 2.8953e-03, 8.3712e-02,\n",
              "         5.3614e-01, 2.0952e-03, 9.9951e-01, 3.4694e-03, 8.7707e-01, 2.7764e-02,\n",
              "         1.0000e+00, 6.8048e-03, 9.0673e-02, 2.6602e-03, 9.9422e-01, 9.9999e-01,\n",
              "         9.1960e-03, 7.2017e-02, 9.9998e-01, 7.7271e-01, 3.5132e-03, 1.3949e-03,\n",
              "         6.3760e-03, 1.6133e-03, 9.9931e-01, 9.9879e-01, 9.9717e-01, 5.7620e-03,\n",
              "         1.6294e-01, 7.4564e-01, 9.9990e-01, 9.9771e-01, 1.0000e+00, 9.9412e-01,\n",
              "         9.9873e-01, 8.2632e-03, 9.9999e-01, 9.9985e-01, 9.9987e-01, 9.9999e-01,\n",
              "         1.8923e-03, 9.9510e-01, 9.9799e-01, 9.6908e-01, 1.9744e-01, 9.9993e-01,\n",
              "         1.0025e-02, 1.6948e-02, 6.9707e-01, 9.0845e-01, 9.9896e-01, 1.0000e+00,\n",
              "         1.1385e-03, 9.9999e-01, 4.6754e-02, 2.0017e-03, 4.8977e-03, 3.1610e-03,\n",
              "         1.0000e+00, 1.7091e-03, 9.9950e-01, 1.5188e-03, 3.3147e-03, 7.8231e-03,\n",
              "         3.0172e-02, 9.9941e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00, 3.6948e-03,\n",
              "         3.9784e-03, 9.9897e-01, 1.4331e-02, 9.9133e-01, 2.5375e-02, 9.9621e-01,\n",
              "         1.0000e+00, 9.9999e-01, 8.6337e-04, 6.6060e-03, 5.3103e-03, 3.7858e-03,\n",
              "         1.5235e-02, 8.2855e-02, 9.8014e-01, 9.8848e-01, 1.4518e-02, 1.0000e+00,\n",
              "         9.6624e-01, 9.1630e-03, 9.9998e-01, 2.4095e-02, 9.9999e-01, 1.1429e-03,\n",
              "         1.8467e-03, 7.6817e-03, 9.1929e-03, 5.4331e-03, 4.2807e-03, 7.9000e-03,\n",
              "         1.1293e-02, 1.2229e-03, 5.6673e-02, 9.9947e-01, 9.9969e-01, 1.0000e+00,\n",
              "         9.9994e-01, 5.2796e-03, 1.1916e-02, 8.6685e-03, 9.9999e-01, 9.9982e-01,\n",
              "         9.9999e-01, 1.8406e-02, 7.1946e-03, 9.9275e-01, 9.9979e-01, 9.2029e-01,\n",
              "         1.0000e+00, 3.6669e-03, 1.5327e-01, 2.9270e-03, 3.6237e-03, 9.9999e-01,\n",
              "         1.0000e+00, 9.5476e-01, 1.0000e+00, 4.9048e-03, 9.9236e-01, 6.2426e-02,\n",
              "         9.9986e-01, 6.2065e-03, 1.8251e-03, 3.7857e-02, 1.0000e+00, 2.6608e-03,\n",
              "         1.3062e-03, 9.1528e-01, 9.9847e-01, 1.3174e-02, 5.1592e-03, 1.3778e-01,\n",
              "         9.9999e-01, 9.8567e-01, 1.2186e-02, 3.9357e-03, 6.3223e-03, 3.4153e-02,\n",
              "         4.1384e-03, 1.0000e+00, 1.3373e-02, 9.9732e-01, 9.9999e-01, 2.6637e-03]],\n",
              "       grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = torch.tensor(y_test.values, dtype=torch.float32)\n",
        "y_test = y_test.unsqueeze(0)\n",
        "y_test = y_test.to(device)\n",
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLwce_KAi27t",
        "outputId": "88e01511-816a-4d0a-cc83-c2aadbb33fdd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
              "         0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
              "         0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0.,\n",
              "         0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
              "         1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
              "         1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
              "         1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1.,\n",
              "         0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
              "         0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
              "         1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
              "         0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0.,\n",
              "         0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
              "         1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
              "         1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
              "         1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0.,\n",
              "         1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for i in range(0, 299):\n",
        "  if(y_test[0][i.long()] == 1 and out[0][i.long()] > 0.5):\n",
        "    count += 1\n",
        "  if(y_test[0][i.long()] == 0 and out[0][i.long()] < 0.5):\n",
        "    count += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIJxBgm8jvX3",
        "outputId": "1e1f35af-1fed-4623-84cc-0a382ef7cf53"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-31868f6fbac8>:2: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  for i in range(0, 299):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('accuracy on test: ')\n",
        "print(count/3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snWxn5HUj2TU",
        "outputId": "727aab03-57ea-4b81-bca1-9795ac3bff23"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy on test: \n",
            "96.33333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = sig(torch.matmul(torch.t(weights), X_train.T))\n",
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4-87axzk9Of",
        "outputId": "1d264cdc-b80d-460b-a2cf-519afd08d0c2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.7779e-03, 9.9792e-01, 1.1914e-02, 9.9911e-01, 9.9999e-01, 2.8051e-01,\n",
              "         1.3697e-02, 9.9998e-01, 1.2814e-03, 2.9369e-03, 9.9776e-01, 9.9950e-01,\n",
              "         9.9998e-01, 6.6899e-03, 1.0000e+00, 2.8136e-02, 2.0651e-02, 2.8909e-03,\n",
              "         6.2604e-02, 1.6373e-03, 9.9998e-01, 9.9877e-01, 9.9986e-01, 9.9999e-01,\n",
              "         4.2659e-03, 2.5703e-03, 1.6336e-02, 9.8437e-01, 3.5748e-03, 4.7222e-03,\n",
              "         5.7390e-03, 7.8989e-02, 9.9933e-01, 2.0737e-03, 6.5272e-03, 1.3439e-01,\n",
              "         9.6409e-01, 4.2790e-03, 2.0545e-02, 2.6351e-01, 9.9999e-01, 9.9999e-01,\n",
              "         1.0427e-02, 2.7402e-03, 9.9999e-01, 9.9996e-01, 1.0000e+00, 3.6393e-02,\n",
              "         9.6450e-01, 7.4064e-01, 5.1211e-01, 4.6492e-03, 7.8254e-02, 9.9829e-01,\n",
              "         5.0797e-03, 9.9998e-01, 6.6213e-03, 9.9980e-01, 9.9991e-01, 4.5391e-02,\n",
              "         9.9998e-01, 9.9999e-01, 9.9999e-01, 9.9753e-01, 1.0000e+00, 9.9986e-01,\n",
              "         9.6354e-01, 1.0000e+00, 7.6270e-03, 9.9999e-01, 1.0413e-02, 2.5461e-03,\n",
              "         1.4411e-02, 2.0502e-01, 9.8927e-03, 9.9990e-01, 9.9991e-01, 9.9999e-01,\n",
              "         1.0000e+00, 8.6984e-03, 9.4251e-01, 1.0000e+00, 7.4198e-02, 9.9956e-01,\n",
              "         4.8996e-03, 1.0000e+00, 9.9994e-01, 8.9536e-01, 1.0000e+00, 9.9998e-01,\n",
              "         1.0000e+00, 1.1829e-02, 9.9999e-01, 9.9999e-01, 2.3912e-03, 8.1619e-01,\n",
              "         7.6055e-03, 9.9997e-01, 9.9914e-01, 1.0000e+00, 9.9885e-01, 9.9778e-01,\n",
              "         1.7975e-03, 9.9999e-01, 9.9999e-01, 1.3357e-02, 2.3399e-02, 4.4881e-03,\n",
              "         3.3679e-03, 9.9993e-01, 5.3689e-03, 3.6316e-03, 9.9908e-01, 7.3390e-03,\n",
              "         9.9800e-01, 1.0000e+00, 9.9996e-01, 9.9999e-01, 1.0000e+00, 2.9116e-02,\n",
              "         9.9999e-01, 9.9989e-01, 9.9995e-01, 1.3893e-01, 1.9574e-03, 3.0743e-02,\n",
              "         9.9999e-01, 4.8323e-03, 9.6135e-01, 4.9686e-03, 7.1771e-03, 9.9964e-01,\n",
              "         9.9987e-01, 9.9999e-01, 1.8278e-01, 9.9970e-01, 9.9999e-01, 7.4041e-03,\n",
              "         2.9112e-03, 1.0000e+00, 9.3410e-01, 3.8322e-03, 9.9984e-01, 7.5364e-02,\n",
              "         9.9998e-01, 9.9969e-01, 9.9997e-01, 9.9999e-01, 9.9901e-01, 9.9987e-01,\n",
              "         5.9258e-02, 1.3038e-03, 6.0385e-02, 2.7265e-01, 1.0000e+00, 9.9992e-01,\n",
              "         1.0000e+00, 9.9102e-01, 7.7507e-03, 1.8176e-01, 9.9994e-01, 1.0000e+00,\n",
              "         7.8510e-03, 1.2430e-02, 1.0743e-03, 7.0140e-02, 3.4326e-03, 2.0617e-03,\n",
              "         6.7086e-01, 7.0962e-03, 1.0000e+00, 2.1395e-03, 2.8588e-03, 9.9998e-01,\n",
              "         2.7697e-02, 8.2540e-02, 9.9964e-01, 9.9996e-01, 9.9999e-01, 3.5037e-02,\n",
              "         9.9982e-01, 9.9996e-01, 1.2678e-03, 1.0000e+00, 1.6789e-02, 4.2315e-03,\n",
              "         3.3167e-03, 9.1429e-01, 2.0250e-03, 1.0000e+00, 9.9991e-01, 1.4423e-03,\n",
              "         9.9984e-01, 1.0000e+00, 9.2388e-01, 9.9997e-01, 9.9999e-01, 9.8817e-01,\n",
              "         9.4340e-01, 8.1308e-03, 4.0270e-03, 2.5913e-02, 2.9573e-02, 9.9994e-01,\n",
              "         9.9971e-01, 9.9987e-01, 9.9999e-01, 3.0821e-03, 9.9969e-01, 1.0104e-03,\n",
              "         9.9992e-01, 3.8495e-03, 9.9994e-01, 9.3879e-01, 8.0782e-03, 9.9941e-01,\n",
              "         9.9909e-01, 5.3491e-03, 1.3903e-03, 1.0000e+00, 1.0000e+00, 1.2051e-03,\n",
              "         9.5288e-03, 9.9997e-01, 4.2228e-03, 1.9051e-03, 9.9990e-01, 2.4948e-02,\n",
              "         4.8944e-03, 9.9952e-01, 7.8377e-02, 2.3759e-03, 9.9996e-01, 9.9999e-01,\n",
              "         7.1971e-03, 2.0140e-03, 8.2124e-03, 3.5594e-02, 1.4106e-01, 9.5346e-01,\n",
              "         8.5106e-01, 3.1594e-03, 9.7719e-01, 5.0846e-03, 9.9932e-01, 9.9996e-01,\n",
              "         1.5794e-03, 1.5392e-02, 9.9994e-01, 9.8950e-01, 9.7899e-01, 5.3991e-03,\n",
              "         9.9959e-01, 1.0000e+00, 1.6716e-03, 1.0000e+00, 1.2181e-02, 9.9818e-01,\n",
              "         3.5993e-02, 5.2746e-01, 9.9600e-01, 9.9979e-01, 6.7167e-03, 1.1441e-03,\n",
              "         1.0000e+00, 9.9996e-01, 1.5922e-03, 1.0000e+00, 9.9973e-01, 9.9997e-01,\n",
              "         9.9872e-01, 9.9749e-01, 2.4517e-01, 9.5926e-03, 6.8113e-03, 5.3087e-01,\n",
              "         9.9919e-01, 9.9415e-01, 3.1266e-03, 7.2896e-01, 9.9786e-01, 9.9975e-01,\n",
              "         9.9999e-01, 9.9990e-01, 9.9999e-01, 9.9995e-01, 3.4391e-02, 1.3343e-02,\n",
              "         9.9996e-01, 6.0246e-03, 8.0069e-01, 9.9970e-01, 2.9208e-03, 7.2412e-03,\n",
              "         9.9999e-01, 9.8174e-03, 5.4503e-03, 9.9988e-01, 1.9254e-03, 9.9974e-01,\n",
              "         4.7599e-02, 4.7970e-02, 4.2731e-02, 7.0898e-03, 1.5782e-01, 1.8260e-02,\n",
              "         9.9997e-01, 1.2726e-03, 9.9999e-01, 1.8877e-02, 1.0000e+00, 9.9171e-01,\n",
              "         6.8254e-01, 1.2516e-01, 7.5147e-01, 1.5426e-03, 2.1403e-03, 9.5981e-01,\n",
              "         3.3751e-03, 9.9997e-01, 9.6577e-03, 8.8534e-01, 9.9995e-01, 7.6971e-02,\n",
              "         1.0000e+00, 2.1822e-03, 9.9996e-01, 1.0000e+00, 1.0000e+00, 9.9991e-01,\n",
              "         9.7599e-01, 1.0285e-01, 4.7428e-03, 5.0353e-01, 3.0408e-02, 9.0798e-01,\n",
              "         9.9989e-01, 1.8177e-01, 5.5557e-03, 9.9971e-01, 8.3121e-03, 1.6420e-02,\n",
              "         3.1186e-02, 9.9998e-01, 5.4639e-03, 5.4268e-03, 2.4160e-02, 1.8635e-03,\n",
              "         1.0196e-02, 1.0000e+00, 2.6524e-03, 8.2822e-02, 3.0350e-02, 9.9997e-01,\n",
              "         6.2512e-01, 9.9937e-01, 1.2789e-02, 1.0000e+00, 4.7116e-03, 1.0000e+00,\n",
              "         1.0000e+00, 9.9995e-01, 5.9560e-03, 5.5127e-03, 3.2998e-01, 9.9989e-01,\n",
              "         4.3519e-03, 2.6966e-02, 7.2465e-03, 2.6564e-01, 9.9960e-01, 4.4508e-03,\n",
              "         3.3058e-02, 3.4158e-02, 9.0668e-01, 9.9989e-01, 8.0925e-02, 1.6056e-02,\n",
              "         1.7383e-02, 7.0435e-03, 1.3760e-02, 2.0890e-03, 9.9999e-01, 1.4286e-03,\n",
              "         1.0000e+00, 9.9984e-01, 2.8399e-03, 1.0863e-03, 9.6834e-01, 8.4514e-01,\n",
              "         8.0898e-03, 1.2503e-03, 4.9530e-01, 2.7852e-02, 8.3683e-01, 9.0177e-01,\n",
              "         9.9991e-01, 4.5109e-03, 3.4734e-03, 2.6351e-01, 1.4110e-02, 9.9998e-01,\n",
              "         9.9986e-01, 4.7621e-03, 1.0000e+00, 1.0000e+00, 9.9925e-01, 9.9956e-01,\n",
              "         2.9115e-03, 9.9998e-01, 5.0177e-03, 1.5608e-03, 7.8646e-02, 9.9998e-01,\n",
              "         9.9998e-01, 1.0000e+00, 9.9942e-01, 4.4002e-03, 9.9777e-01, 4.3747e-03,\n",
              "         1.2305e-03, 1.0908e-03, 9.7745e-02, 3.6537e-03, 2.4823e-03, 9.9995e-01,\n",
              "         5.2847e-03, 5.9770e-03, 9.9999e-01, 9.9998e-01, 9.9994e-01, 1.0000e+00,\n",
              "         9.9858e-01, 9.9995e-01, 9.9984e-01, 5.7197e-03, 9.9994e-01, 1.1754e-02,\n",
              "         1.3849e-02, 1.0000e+00, 9.9996e-01, 5.0446e-03, 4.9390e-03, 2.7278e-03,\n",
              "         1.0000e+00, 1.5159e-01, 9.9994e-01, 3.2586e-02, 9.9943e-01, 5.1913e-02,\n",
              "         5.8843e-03, 1.1943e-01, 7.1947e-03, 5.5839e-03, 1.0000e+00, 9.8762e-04,\n",
              "         4.8484e-03, 2.5288e-02, 2.4955e-03, 1.5306e-03, 2.4535e-02, 3.1393e-03,\n",
              "         9.5231e-01, 3.6239e-03, 3.3712e-03, 9.9226e-01, 9.9998e-01, 1.5786e-01,\n",
              "         2.4007e-02, 6.6279e-03, 9.9999e-01, 9.9902e-01, 2.7493e-03, 3.4317e-02,\n",
              "         9.9997e-01, 9.9999e-01, 9.9940e-01, 1.0000e+00, 4.6828e-03, 9.9821e-01,\n",
              "         9.9995e-01, 6.1132e-03, 9.9999e-01, 3.5963e-03, 9.9995e-01, 2.2242e-01,\n",
              "         9.9950e-01, 6.6409e-03, 1.3840e-02, 9.9985e-01, 4.7581e-03, 6.0568e-02,\n",
              "         9.9997e-01, 1.6571e-02, 2.2685e-03, 8.1371e-03, 7.5172e-01, 1.0000e+00,\n",
              "         9.9989e-01, 9.9999e-01, 9.9997e-01, 6.3981e-03, 3.0191e-03, 2.8249e-03,\n",
              "         9.9999e-01, 4.9183e-03, 4.8987e-02, 3.0051e-01, 9.6357e-04, 7.3148e-03,\n",
              "         1.6651e-03, 1.0000e+00, 1.8077e-03, 2.8577e-03, 3.6806e-03, 1.2648e-01,\n",
              "         9.9966e-01, 1.0000e+00, 8.2326e-03, 6.6792e-01, 9.9958e-01, 9.1937e-03,\n",
              "         1.0000e+00, 1.0919e-02, 1.0000e+00, 9.9988e-01, 2.1211e-03, 4.8096e-03,\n",
              "         3.0155e-03, 1.0000e+00, 3.0367e-01, 9.4186e-01, 6.5198e-03, 9.9998e-01,\n",
              "         1.0682e-02, 9.9980e-01, 3.3888e-03, 9.9878e-01, 5.2048e-02, 7.6124e-01,\n",
              "         5.2701e-03, 5.2535e-03, 1.5277e-02, 9.9999e-01, 1.0000e+00, 1.0806e-03,\n",
              "         7.5086e-01, 9.9861e-01, 1.2365e-02, 3.1387e-02, 9.9999e-01, 3.6847e-03,\n",
              "         9.9998e-01, 4.7580e-02, 1.0000e+00, 9.9998e-01, 9.9998e-01, 3.9289e-03,\n",
              "         1.9356e-03, 1.1816e-02, 9.8039e-01, 9.9980e-01, 1.5008e-02, 5.4941e-03,\n",
              "         1.8758e-02, 3.0621e-03, 8.0467e-02, 6.4785e-02, 1.3418e-01, 2.2619e-02,\n",
              "         9.9902e-01, 6.2079e-03, 9.9997e-01, 9.9983e-01, 1.0000e+00, 3.1330e-02,\n",
              "         9.9986e-01, 2.4761e-02, 1.0000e+00, 9.9996e-01, 8.8051e-03, 6.3586e-02,\n",
              "         1.9597e-03, 9.4554e-04, 9.9950e-01, 9.9999e-01, 1.0000e+00, 9.9980e-01,\n",
              "         1.4905e-02, 1.3641e-03, 9.9999e-01, 9.9960e-01, 3.1190e-03, 1.5003e-03,\n",
              "         9.9960e-01, 9.9900e-01, 5.2480e-03, 9.9999e-01, 8.3076e-03, 9.9920e-01,\n",
              "         1.8543e-03, 2.2189e-01, 7.6055e-03, 3.5117e-03, 1.1676e-03, 8.1865e-03,\n",
              "         1.5638e-03, 3.5728e-03, 5.4725e-03, 7.0871e-03, 2.8970e-03, 2.4889e-01,\n",
              "         1.9997e-03, 9.5753e-03, 9.9999e-01, 4.4610e-03, 9.9987e-01, 9.9412e-01,\n",
              "         1.0000e+00, 2.3409e-03, 9.9992e-01, 9.9999e-01, 1.0000e+00, 9.9188e-01,\n",
              "         2.4397e-03, 9.7001e-01, 6.9962e-03, 9.7372e-01, 3.4426e-02, 9.9838e-01,\n",
              "         9.9703e-01, 9.9021e-02, 9.3669e-01, 9.9996e-01, 1.1007e-02, 9.9944e-01,\n",
              "         9.8714e-01, 1.0000e+00, 1.7095e-01, 1.0000e+00, 9.9815e-01, 9.9150e-01,\n",
              "         1.0000e+00, 4.2343e-03, 9.9999e-01, 9.9996e-01, 9.9999e-01, 9.9988e-01,\n",
              "         6.0724e-03, 1.0000e+00, 9.9995e-01, 4.5125e-02, 1.4500e-03, 1.0000e+00,\n",
              "         3.1268e-03, 3.5859e-03, 1.7363e-03, 9.9959e-01, 9.9990e-01, 3.3207e-03,\n",
              "         9.9999e-01, 9.9999e-01, 1.1634e-03, 9.9999e-01, 2.2506e-03, 1.3108e-03,\n",
              "         1.5082e-03, 3.0210e-03, 7.3401e-03, 5.3325e-03, 1.0000e+00, 9.9997e-01,\n",
              "         3.9885e-03, 1.1248e-02, 1.0000e+00, 9.9999e-01, 3.0501e-03, 5.4545e-02,\n",
              "         1.9049e-02, 6.3459e-01, 1.0000e+00, 9.9998e-01, 3.6985e-03, 1.9924e-03,\n",
              "         6.9404e-03, 9.9941e-01, 9.8107e-01, 1.2099e-02, 5.1257e-03, 9.9207e-01,\n",
              "         4.1140e-01, 5.5336e-03, 1.2657e-03, 4.9338e-01, 7.1446e-02, 6.4565e-03,\n",
              "         9.9972e-01, 1.4696e-01, 9.9562e-01, 3.2588e-02]],\n",
              "       grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2F9aM0cmrAc",
        "outputId": "61fc6f65-82fc-4776-e6c5-5892817b82d1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
              "         0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "         1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
              "         0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
              "         0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
              "         1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0.,\n",
              "         0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0.,\n",
              "         1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0.,\n",
              "         1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
              "         1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
              "         1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1.,\n",
              "         1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1.,\n",
              "         0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
              "         1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,\n",
              "         1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
              "         1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
              "         1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
              "         1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
              "         0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
              "         1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0.,\n",
              "         0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
              "         1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1.,\n",
              "         1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
              "         1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
              "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
              "         0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0.,\n",
              "         1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
              "         1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
              "         1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
              "         0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
              "         0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
              "         1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
              "         1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "         0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
              "         1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
              "         0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
              "         0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
              "         0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for i in range(0, 699):\n",
        "  if(y_train[0][i.long()] == 1 and out[0][i.long()] > 0.5):\n",
        "    count += 1\n",
        "  if(y_train[0][i.long()] == 0 and out[0][i.long()] < 0.5):\n",
        "    count += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFKIW5Ammsqn",
        "outputId": "8687de17-9010-4467-ac42-363f2e027563"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-90bdf584d63d>:2: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  for i in range(0, 699):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('accuracy on train: ')\n",
        "print(count/7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfP-h2cMmzAC",
        "outputId": "c804fd7a-e2f7-43f9-9c56-5e7b32629503"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy on train: \n",
            "97.42857142857143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ptx6hyh6nGCT"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}